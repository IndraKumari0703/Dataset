{"cells":[{"cell_type":"code","execution_count":null,"id":"84d04e92-7385-4cd8-bd5f-edeae6923294","metadata":{"id":"84d04e92-7385-4cd8-bd5f-edeae6923294","outputId":"c8cd4019-cba2-4329-db59-dc13e82e26d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n","C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"id":"2e81542b-3bea-43c7-9819-f07d60a02ea0","metadata":{"id":"2e81542b-3bea-43c7-9819-f07d60a02ea0"},"outputs":[],"source":["# Step 1: Load the pre-trained model\n","model = load_model(\"/content/drive/MyDrive/ProfessorProject/2ChartClassification/Chart.h5\")"]},{"cell_type":"code","execution_count":null,"id":"04713278-0da2-48b3-a64d-972aa1089c99","metadata":{"id":"04713278-0da2-48b3-a64d-972aa1089c99"},"outputs":[],"source":["# Step 2: Define paths to the evaluation data folders\n","evaluation_FigQA_folder = \"\""]},{"cell_type":"code","execution_count":null,"id":"8492ed9d-07cb-4532-b0b5-9c1ca8e2e062","metadata":{"id":"8492ed9d-07cb-4532-b0b5-9c1ca8e2e062","outputId":"aa44b460-a22c-45f0-dd96-0a90d57709ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of images per class: \n","Class: bar, Number of images: 15581\n","Class: line, Number of images: 2195\n","Class: pie, Number of images: 541\n"]}],"source":["# Total images in FigQA folder\n","classes = os.listdir(evaluation_FigQA_folder)\n","print(\"Total number of images per class: \")\n","for class_name in classes:\n","    class_path = os.path.join(evaluation_FigQA_folder, class_name)\n","    num_images = len(os.listdir(class_path))\n","    print(f\"Class: {class_name}, Number of images: {num_images}\")"]},{"cell_type":"code","execution_count":null,"id":"ac28227f-ca16-43dd-8fa7-3a992851cb71","metadata":{"id":"ac28227f-ca16-43dd-8fa7-3a992851cb71","outputId":"dfc34d9a-d577-4eca-f248-8f60cac40629"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 18317 images belonging to 3 classes.\n"]}],"source":["# Step 3: Prepare the evaluation data\n","img_width, img_height = 224, 224  # Adjust the image size based on your model requirements\n","batch_size = 32\n","\n","# Use ImageDataGenerator to load and preprocess the evaluation images\n","datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n","\n","\n","\n","if len(evaluation_FigQA_folder) != 0:\n","  evaluation_raw_generator = datagen.flow_from_directory(\n","      evaluation_FigQA_folder,\n","      target_size=(img_width, img_height),\n","      batch_size=batch_size,\n","      class_mode='categorical',\n","      shuffle=False\n","  )\n"]},{"cell_type":"code","execution_count":null,"id":"e7a67362-a770-4da0-bf9b-d305f81ca966","metadata":{"id":"e7a67362-a770-4da0-bf9b-d305f81ca966","outputId":"b705215e-542c-4f23-a5e7-c88aca541606"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_884\\3911544973.py:11: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  predictions = model.predict_generator(evaluation_raw_generator, steps=steps_per_epoch)\n"]},{"name":"stdout","output_type":"stream","text":["14428 15581 bar\n","2192 2195 line\n","541 541 pie\n"]}],"source":["# Step 4: Evaluate the model\n","# 4.1 Evaluation on FigQA dataset\n","if len(evaluation_FigQA_folder) != 0:\n","  num_samples = evaluation_raw_generator.samples\n","  steps_per_epoch = int(np.ceil(num_samples / batch_size))\n","\n","  # Get the class labels in the correct order (important for per class accuracy)\n","  class_labels = list(evaluation_raw_generator.class_indices.keys())\n","\n","  # Evaluate the model and make predictions\n","  predictions = model.predict_generator(evaluation_raw_generator, steps=steps_per_epoch)\n","  predicted_labels = np.argmax(predictions, axis=1)\n","\n","  # Get the ground-truth labels\n","  ground_truth_labels = evaluation_raw_generator.classes\n","\n","  # Calculate per-class accuracy\n","  per_class_accuracy_Raw = {}\n","  for i, class_label in enumerate(class_labels):\n","      class_indices = np.where(ground_truth_labels == i)[0]\n","      correct_predictions = np.sum(predicted_labels[class_indices] == i)\n","      total_samples = len(class_indices)\n","      per_class_accuracy_Raw[class_label] = correct_predictions / total_samples"]},{"cell_type":"code","execution_count":null,"id":"433ed920-70bf-4194-809b-53e5624a29d6","metadata":{"id":"433ed920-70bf-4194-809b-53e5624a29d6","outputId":"d848ed94-dc6c-4de2-983b-9ead3da0172b"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_884\\395925203.py:4: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n","  evaluation_results = model.evaluate_generator(evaluation_raw_generator, steps=steps_per_epoch)\n"]}],"source":["# Step 5: Extract evaluation metrics\n","# 5.1: On FigQA dataset\n","if len(evaluation_FigQA_folder) != 0:\n","  evaluation_results = model.evaluate_generator(evaluation_raw_generator, steps=steps_per_epoch)\n","  loss_rawData = evaluation_results[0]\n","  accuracy_rawData = evaluation_results[1]"]},{"cell_type":"code","execution_count":null,"id":"fdf8d427-08e2-46ea-a179-73fed0507576","metadata":{"id":"fdf8d427-08e2-46ea-a179-73fed0507576","outputId":"192b2459-d2c4-4295-9f73-1a4be93a288b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluation Summary on ChartQA dataset:\n","Overall Loss: 0.14707538485527039\n","Overall Accuracy: 0.9368892312049866\n","Per Class Accuracy:\n","bar: 0.9259996149156023\n","line: 0.9986332574031891\n","pie: 1.0\n","\n","\n"]}],"source":["\n","# Step 6: Print the evaluation summary\n","# 6.1: On FigQA dataset\n","if len(evaluation_FigQA_folder) != 0:\n","  print(\"Evaluation Summary on FigQA dataset:\")\n","  print(\"Overall Loss:\", loss_rawData)\n","  print(\"Overall Accuracy:\", accuracy_rawData)\n","  print(\"Per Class Accuracy:\")\n","  for class_name, acc in per_class_accuracy_Raw.items():\n","      print(f\"{class_name}: {acc}\")\n","  print('')\n","  print('')"]},{"cell_type":"code","execution_count":null,"id":"2519d554-911f-466c-b1ae-a85a92beb70e","metadata":{"id":"2519d554-911f-466c-b1ae-a85a92beb70e"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[{"file_id":"1l2702neWfwr2c-Y3vlvgei6CP4XYfpqP","timestamp":1691920006094}]}},"nbformat":4,"nbformat_minor":5}